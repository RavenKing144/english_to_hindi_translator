# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mwvf4R8GOmNPQ2BklZmS0XN0wCkrtSHv

# Stage 1: Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import re
import string
import numpy as np
import pandas as pd
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, RepeatVector, Embedding, Dropout

"""# Stage 2: Mounting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Stage 3: loading Training and Validation Dataset"""

train_df = pd.read_xml("/content/drive/MyDrive/en2hi/training.xml")
val_df = pd.read_xml("/content/drive/MyDrive/en2hi/validation.xml")

"""## Stage 3.1: Divinding train-validation languages"""

train_eng = list(train_df.SourceName)
train_hi = list(train_df.TargetName)
val_eng = list(val_df.SourceName)
val_hi = list(val_df.TargetName)

"""## Stage 3.2: Data Preprocessing

"""

train_eng = [i.lower() for i in train_eng]
val_eng = [i.lower() for i in val_eng]

"""## Stage 3.3: Text to Sequence conversion"""

def tokenizer(lines):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(lines)
  return tokenizer
eng_train_token = tokenizer(train_eng)
eng_val_token = tokenizer(val_eng)
hin_train_token = tokenizer(train_hi)
hi_val_token = tokenizer(val_hi)

eng_vocab_size = len(eng_train_token.word_index)+1
hi_vocab_size = len(hin_train_token.word_index)+1
max_eng_length = 8
max_hi_length = 8

def encode(tokenizer, length, line):
  seq = tokenizer.texts_to_sequences(line)
  seq = pad_sequences(seq, maxlen=length, padding = 'post')
  return seq

"""# Stage 4: Train, Test split"""

trainX = encode(eng_train_token, max_eng_length, train_eng)
trainY = encode(hin_train_token, max_hi_length, train_hi)
testX = encode(eng_val_token, max_eng_length, val_eng)
testY = encode(hi_val_token, max_hi_length, val_hi)

"""# Stage 5: Defining basic Seq2Seq Model"""

def define_model(in_vocab, out_vocab, in_time, out_time, units):
  model = Sequential()
  model.add(Embedding(in_vocab, units, input_length = in_time, mask_zero=True))
  model.add(LSTM(units))
  model.add(RepeatVector(out_time))
  model.add(LSTM(units, return_sequences = True))
  model.add(Dense(out_vocab, activation = 'softmax'))
  model.add(Dropout(rate = 0.2))
  return model

model = define_model(eng_vocab_size, hi_vocab_size, max_eng_length, max_hi_length, 512)
rms = optimizers.RMSprop(lr=0.001)
model.compile(optimizer = rms, loss = 'sparse_categorical_crossentropy')

model.summary()

"""# Stage 6: Training"""

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/en2hi/checkpoints/',
                                                 verbose=1, monitor='val_accuracy',
                      mode='max',
    save_best_only=True)
history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1],1), epochs =200, batch_size =32, validation_split=0.1, callbacks=[cp_callback])

